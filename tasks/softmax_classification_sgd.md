### Assignment: softmax_classification_sgd
#### Date: Deadline: Nov 10, 23:59
#### Points: 3 points
#### Examples: softmax_classification_sgd_examples

Starting with the [softmax_classification_sgd.py](https://github.com/ufal/npfl129/tree/master/labs/04/softmax_classification_sgd.py),
implement minibatch SGD for multinomial logistic regression.

#### Examples Start: softmax_classification_sgd_examples
Note that your results may sometimes be slightly different (for example because of varying floating point arithmetic on your CPU).
- `python3 softmax_classification_sgd.py --batch_size=10  --iterations=10 --learning_rate=0.005`
```
After iteration 1: train loss 0.2428 acc 93.4%, test loss 0.2996 acc 90.8%
After iteration 2: train loss 0.1962 acc 94.6%, test loss 0.2649 acc 92.5%
After iteration 3: train loss 0.1746 acc 95.2%, test loss 0.2595 acc 91.5%
After iteration 4: train loss 0.1268 acc 96.8%, test loss 0.2074 acc 92.1%
After iteration 5: train loss 0.1013 acc 97.4%, test loss 0.1861 acc 93.9%
After iteration 6: train loss 0.0950 acc 98.4%, test loss 0.1754 acc 93.7%
After iteration 7: train loss 0.0810 acc 98.1%, test loss 0.1587 acc 94.9%
After iteration 8: train loss 0.0761 acc 98.2%, test loss 0.1564 acc 94.9%
After iteration 9: train loss 0.0764 acc 98.3%, test loss 0.1654 acc 94.9%
After iteration 10: train loss 0.0746 acc 98.2%, test loss 0.1694 acc 95.2%
Learned weights:
  -0.03 -0.10 0.01 0.07 -0.03 0.03 -0.07 0.05 0.07 -0.10 ...
  0.09 0.08 -0.12 -0.07 -0.20 0.10 -0.02 -0.06 0.02 -0.07 ...
  0.05 0.07 0.01 0.01 -0.03 0.02 0.01 -0.10 -0.03 0.10 ...
  0.02 -0.05 -0.03 0.09 0.17 0.14 -0.02 0.05 -0.09 0.04 ...
  -0.07 -0.07 -0.11 -0.06 -0.09 -0.09 -0.10 0.03 -0.04 0.01 ...
  -0.07 -0.04 0.18 0.02 0.04 0.14 0.10 0.03 -0.03 0.02 ...
  -0.09 -0.04 -0.12 -0.08 -0.08 -0.12 -0.08 0.05 0.05 -0.05 ...
  0.07 0.02 0.04 -0.02 0.10 0.01 0.16 -0.04 0.03 0.01 ...
  0.02 -0.02 0.02 -0.05 -0.02 -0.03 -0.10 -0.03 0.08 -0.07 ...
  0.04 -0.06 -0.07 0.10 -0.04 -0.05 0.06 -0.08 -0.01 -0.01 ...
```
- `python3 softmax_classification_sgd.py --batch_size=1   --iterations=10 --learning_rate=0.005`
```
After iteration 1: train loss 0.9946 acc 89.7%, test loss 1.2550 acc 88.1%
After iteration 2: train loss 0.2859 acc 95.0%, test loss 0.5081 acc 93.2%
After iteration 3: train loss 0.6259 acc 91.7%, test loss 1.0368 acc 89.7%
After iteration 4: train loss 0.5527 acc 91.9%, test loss 0.9937 acc 87.5%
After iteration 5: train loss 0.6618 acc 92.2%, test loss 1.1615 acc 89.2%
After iteration 6: train loss 0.1897 acc 97.0%, test loss 0.5966 acc 93.2%
After iteration 7: train loss 0.1943 acc 97.2%, test loss 0.6839 acc 93.0%
After iteration 8: train loss 0.3295 acc 95.0%, test loss 0.6580 acc 92.8%
After iteration 9: train loss 0.1317 acc 97.6%, test loss 0.5526 acc 92.8%
After iteration 10: train loss 0.2316 acc 96.3%, test loss 0.5620 acc 94.6%
Learned weights:
  -0.03 -0.11 0.03 0.27 0.07 -0.06 -0.18 0.04 0.07 -0.17 ...
  0.09 0.07 -0.19 -0.10 -1.01 0.59 0.04 -0.09 0.00 -0.74 ...
  0.05 0.11 0.15 0.28 0.12 -0.03 -0.15 -0.10 -0.03 0.36 ...
  0.02 -0.01 -0.19 0.27 0.63 0.55 -0.07 0.03 -0.09 0.19 ...
  -0.07 -0.09 -0.34 -0.62 -0.47 -0.49 -0.31 -0.03 -0.04 0.02 ...
  -0.07 0.07 0.87 -0.27 0.16 0.65 0.57 -0.06 -0.03 0.04 ...
  -0.09 -0.09 -0.42 -0.17 -0.24 -0.60 -0.12 0.05 0.05 -0.30 ...
  0.07 0.12 0.21 0.16 0.63 0.11 0.66 0.18 0.03 0.36 ...
  0.02 -0.11 0.04 -0.24 -0.20 -0.38 -0.51 -0.04 0.10 -0.12 ...
  0.04 -0.18 -0.36 0.44 0.12 -0.18 0.01 -0.07 -0.01 0.26 ...
```
- `python3 softmax_classification_sgd.py --batch_size=100 --iterations=10 --learning_rate=0.05`
```
After iteration 1: train loss 4.6130 acc 66.4%, test loss 4.7159 acc 66.5%
After iteration 2: train loss 0.4662 acc 91.0%, test loss 0.5955 acc 87.1%
After iteration 3: train loss 0.4140 acc 90.2%, test loss 0.6039 acc 87.5%
After iteration 4: train loss 0.2332 acc 93.3%, test loss 0.4390 acc 90.0%
After iteration 5: train loss 0.1398 acc 96.2%, test loss 0.2685 acc 91.8%
After iteration 6: train loss 0.0965 acc 96.9%, test loss 0.2300 acc 93.7%
After iteration 7: train loss 0.1034 acc 96.7%, test loss 0.2712 acc 92.5%
After iteration 8: train loss 0.2048 acc 93.8%, test loss 0.4191 acc 90.5%
After iteration 9: train loss 0.8357 acc 84.6%, test loss 0.9188 acc 84.1%
After iteration 10: train loss 0.0825 acc 97.7%, test loss 0.2471 acc 94.0%
Learned weights:
  -0.03 -0.11 -0.02 0.08 -0.08 -0.04 -0.10 0.05 0.07 -0.12 ...
  0.09 0.07 -0.15 -0.14 -0.27 0.15 -0.05 -0.07 0.02 -0.11 ...
  0.05 0.10 0.10 0.03 -0.04 0.05 0.03 -0.10 -0.03 0.20 ...
  0.02 -0.04 -0.00 0.12 0.26 0.24 -0.02 0.05 -0.09 0.08 ...
  -0.07 -0.07 -0.15 -0.14 -0.08 -0.14 -0.13 0.03 -0.04 0.00 ...
  -0.07 -0.03 0.29 0.07 0.10 0.27 0.20 0.03 -0.03 0.01 ...
  -0.09 -0.05 -0.27 -0.12 -0.21 -0.36 -0.15 0.05 0.04 -0.12 ...
  0.07 0.01 0.05 -0.00 0.14 0.09 0.23 -0.04 0.03 0.01 ...
  0.02 -0.03 0.02 -0.04 0.02 -0.06 -0.16 -0.03 0.08 -0.06 ...
  0.04 -0.06 -0.05 0.15 -0.03 -0.06 0.07 -0.07 -0.01 -0.01 ...
```
#### Examples End:
