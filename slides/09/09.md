title: NPFL129, Lecture 9
class: title, cc-by-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Decision Trees, Random Forests

## Jindřich Libovický <small>(reusing materials by Milan Straka)</small>

### November 28, 2023

---
class: middle
# Today's Lecture Objectives

After this lecture you should be able to

- Implement Decision Trees and Random Forests for classification and regression

- Explain how the splitting criterion depend on optimized loss function

- Tell how Random Forests differ from Gradient Boosted Decision Trees

---
section: DecisionTree
# Decision Trees

The idea of decision trees is to partition the input space into regions and
solving each region with a simpler model.

~~~
We focus on **Classification and Regression Trees** (CART; Breiman et al.,
1984), but there are additional variants like ID3, C4.5, …

~~~
![w=80%,mw=49%,h=center](tree_partitioning.svgz)
~~~
![w=90%,mw=49%,h=center](tree_representation.svgz)

---
# Inference and Training

## Inference

![w=30%,f=right](tree_meme.jpg)

- Just follow the branching rules until you reach a leaf.

- Output a prediction (real value/distribution/predicted class) based on the leaf.

~~~
## Training

- Training data is stored in tree leaves -- the leaf prediction is based on what is data items are in the leaf.

- At the beginning the tree is a single leaf node.

- Adding a node = leaf $\rightarrow$ decision node + 2 leafs

- The goal of training = finding the most consistent leafs for the prediction

Later, we will show that the consistency measures follow from the loss function, we are optimizing.

---
# Regression Decision Trees

Assume we have an input dataset $⇉X ∈ ℝ^{N×D}$, $→t ∈ ℝ^N$.
~~~
At the beginning, the decision tree is just a single node and all input examples
belong to this node.
~~~
We denote $I_𝓣$ the set of training example indices belonging to a node $𝓣$.

~~~
For each leaf (a node without children), our model predicts the average of the
training examples belonging to that leaf, $t̂_𝓣 = \frac{1}{|I_𝓣|} ∑_{i ∈ I_𝓣} t_i$.

~~~
We use a **criterion** $c_𝓣$ telling us how _uniform_ or _homogeneous_ the
training examples of a node $𝓣$ are
~~~
– for regression, we employ the sum of squares error between the examples
belonging to the node and the predicted value in that node; this is proportional
to the variance of the training examples belonging to the node $𝓣$, multiplied
by the number of the examples. Note that even if it is not _mean_ squared error,
it is sometimes denoted as MSE.
$$c_\textrm{SE}(𝓣) ≝ ∑_{i ∈ I_𝓣} (t_i - t̂_𝓣)^2\textrm{, ~where~ } t̂_𝓣=\frac{1}{|I_𝓣|} ∑_{i ∈ I_𝓣} t_i.$$

---
class: middle
# Tree Construction

To split a node, the goal is to find

1. A feature and <small>(i.e., a for loop over all features)</small>

2. Its value <small>(i.e., a for loop over all unique feature values)</small>

such that when splitting a node $𝓣$ into $𝓣_L$ and $𝓣_R$, the resulting regions
decrease the overall criterion value the most, i.e., the difference $c_{𝓣_L} +
c_{𝓣_R} - c_𝓣$ is the lowest.


---
# Tree Construction: Heuristics
We usually employ several constraints, the most common ones are:
- **maximum tree depth**: we do not split nodes with this depth;

~~~
- **minimum examples to split**: we only split nodes with this many training
  examples;
~~~
- **maximum number of leaf nodes**: we split until we reach the given number of
  leaves.

~~~
The tree is usually built in one of two ways:
- if the number of leaf nodes is unlimited, we usually build the tree in
  a depth-first manner, recursively splitting every leaf until one
  of the above constraints is invalidated;
~~~
- if the maximum number of leaf nodes is given, we usually split such leaf $𝓣$
  where the criterion difference $c_{𝓣_L} + c_{𝓣_R} - c_𝓣$ is the lowest.

~~~
<br />

_Terminological note:_ Decision tree with unlimited size can be considered a
non-parametric model: it is a way of building an index. With a limited size,
it has a fixed number of parameters to be learned and it can be considered a
parametric model.

---
# Classification Decision Trees

For multi-class classification, we predict the class which is the most frequent
in the training examples belonging to a leaf $𝓣$.

~~~
To define the criteria, let us denote the average probability for class $k$ in
a region $𝓣$ as $p_{𝓣}(k)$.

~~~
For classification trees, one of the following two criteria is usually used:

- **Gini index**, also called **Gini impurity**, measuring how often a randomly
  chosen element would be incorrectly labeled if it was randomly labeled
  according to $→p_𝓣$:
  $$c_\textrm{Gini}(𝓣) ≝ |I_𝓣| ∑_k p_𝓣(k) \big(1 - p_𝓣(k)\big),$$

~~~
- **Entropy Criterion**
  $$c_\textrm{entropy}(𝓣) ≝ |I_𝓣| ⋅ H(→p_𝓣) = - |I_𝓣| ∑_{\substack{k\\p_𝓣(k) ≠ 0}} p_𝓣(k) \log p_𝓣(k).$$

---
# From Loss Function to Splitting Criterion

~~~
- Training GLMs and MLPs is formulated as optimizing a loss function.

~~~
- For an already constructed decision tree, we can do it the same way. For each
  leaf, do the optimization and find the best parameter.

~~~
- So far, we were always interested in $\arg\min$, i.e., parameters that
  minimize the loss.

~~~
- If we plug the $\arg\min$ value in the loss function, we get the minimum
  reachable loss for the given tree structure.

~~~
- By splitting a leaf, we want to decrease the minimum reachable loss
  $\Rightarrow$ the **minimum node loss is the splitting criterion**.

---
section: Gini and Entropy Losses
# Binary Gini as (M)SE Loss

Recall that $I_𝓣$ denotes the set of training example indices belonging to a leaf node $𝓣$,
let $n_𝓣(0)$ be the number of examples with target value 0, $n_𝓣(1)$ be the
number of examples with target value 1, and let $p_𝓣 = \frac{1}{|I_𝓣|} ∑_{i ∈ I_𝓣} t_i = \frac{n_𝓣(1)}{n_𝓣(0) + n_𝓣(1)}$.

~~~
Consider sum of squares loss $L(p) = ∑_{i ∈ I_𝓣} (p - t_i)^2$.

~~~
By setting the derivative of the loss to zero, we get that the $p$ minimizing
the loss fulfills $|I_𝓣|p = ∑_{i ∈ I_𝓣} t_i$, i.e., $p = p_𝓣$.

~~~
The value of the loss is then

$\displaystyle L(p_𝓣) = ∑_{i ∈ I_𝓣} (p_𝓣 - t_i)^2$
~~~
$= n_𝓣(0) \textcolor{blue}{(p_𝓣 - 0)^2} + n_𝓣(1) \textcolor{magenta}{(p_𝓣 - 1)^2}$

~~~
$\displaystyle \phantom{L(p_𝓣)}
  = \frac{n_𝓣(0) \textcolor{blue}{n_𝓣(1)^2}}{\textcolor{blue}{\big(n_𝓣(0) + n_𝓣(1)\big)^2}}
    + \frac{n_𝓣(1) \textcolor{magenta}{n_𝓣(0)^2}}{\textcolor{magenta}{\big(n_𝓣(0) + n_𝓣(1)\big)^2}}$
~~~
$\displaystyle = \mathrlap{
  \frac{(n_𝓣(1) + n_𝓣(0)) \textcolor{green}{n_𝓣(0)} \textcolor{red}{n_𝓣(1)}}
       {\textcolor{green}{\big(n_𝓣(0) + n_𝓣(1)\big)} \textcolor{red}{\big(n_𝓣(0) + n_𝓣(1)\big)}}}$

~~~
$\displaystyle \phantom{L(p_𝓣)} = \big(n_𝓣(0) + n_𝓣(1)\big) \textcolor{green}{(1-p_𝓣)} \textcolor{red}{p_𝓣} = |I_𝓣| ⋅ p_𝓣 (1-p_𝓣).$

---
# Entropy as NLL Loss

Again let $I_𝓣$ denote the set of training example indices belonging to a leaf node $𝓣$,
let $n_𝓣(k)$ be the number of examples with target value $k$, and let
$p_𝓣(k) = \frac{1}{|I_𝓣|} ∑_{i ∈ I_𝓣} [t_i = k] = \frac{n_𝓣(k)}{|I_𝓣|}$.

~~~
Consider a distribution $→p$ on $K$ classes and non-averaged NLL loss $L(→p) = ∑_{i ∈ I_𝓣} - \log p_{t_i}$.

~~~
By setting the derivative of the loss with respect to $p_k$ to zero (using
a Lagrangian with constraint $∑_k p_k = 1$), we get that the $→p$ minimizing the
loss fulfills $p_k = p_𝓣(k)$.

~~~
The value of the loss with respect to $→p_𝓣$ is then

$\displaystyle \kern10em\mathllap{L(→p_𝓣)} = ∑_{i ∈ I_𝓣} - \log p_{t_i}$

~~~
$\displaystyle \kern10em{} = - ∑_{\substack{k\\p_𝓣(k)≠0}} n_𝓣(k) \log p_𝓣(k)$

~~~
$\displaystyle \kern10em{} = - |I_𝓣| ∑_{\substack{k\\p_𝓣(k)≠0}}  p_𝓣(k) \log p_𝓣(k) = |I_𝓣| ⋅ H(→p_𝓣).$


---
section: RandomForests
# Random Forests

Bagging of data combined with a random subset of features (sometimes
called _feature bagging_).

![w=80%,h=center](random_forest.svgz)

---
# Random Forests

## Bagging

Every decision tree is trained using bagging (on a bootstrapped dataset).

~~~
## Random Subset of Features

During each node split, only a random subset of features is considered when
finding the best split. A fresh random subset is used for every node.

~~~
## Extra Trees

The so-called extra trees are even more randomized, not finding the best
possible feature value when choosing a split, but considering uniformly
random samples from a feature's empirical range (minimum and maximum in the
training data).

~~~
## Demo

https://cs.stanford.edu/~karpathy/svmjs/demo/demoforest.html

---
section: Gradient Boosting
# Gradient Boosting Decision Trees: Overview

~~~
- A collection of decision trees: each tree fixes the error of the previous trees

~~~
- Decreasing error = step in the direction of the error/loss gradient

~~~
- GBDT repeat SGD computation at the inference time (start with a stupid
  estimate and do gradient steps that improve the estimate)

~~~
- One tree is one gradient descent step

## Theoretical steps to be taken

~~~
- Derive what exactly we want to the intermediate steps to predict, so we do
  more clever steps than SGD that just moves with gradient

~~~
- Based on that, express the per-node loss and splitting criterion as the
  minimum reachable loss value

<center>For regression, binary classification and multiclass classification.</center>

---
# Gradient Boosting Decision Trees

GBDT are trained sequentially to correct the errors of the previous trees.

<br />

~~~
![w=70%,f=right,mh=80%,v=middle](gbt_example.svgz)

If we denote $y_t$ as the prediction function of the $t^\mathrm{th}$
tree, the prediction of the whole collection is then
$$y(→x_i) = ∑_{t=1}^T y_t(→x_i; →w_t),$$
where $→w_t$ is a vector of parameters (leaf values, to be concrete) of the
$t^\mathrm{th}$ tree.

---
# Gradient Boosting for Regression

Considering a regression task first, we define the overall loss as

$$E(→w) = ∑_i \ell\big(t_i, y(→x_i; →w)\big) + ∑_{t=1}^T \frac{1}{2} λ \big\|→w_t\big\|^2,$$
where
~~~
- $→w = (→w_1, …, →w_T)$ are the parameters (leaf values) of the trees;

~~~
- $\ell\big(t_i, y(→x_i; →w)\big)$ is an per-example loss, $(t_i - y(→x_i;
  →w))^2$ for regression;
~~~
- the $λ$ is the usual $L^2$-regularization strength.

---
# Gradient Boosting for Regression

To construct the trees sequentially, we extend the definition to

$$E^{(t)}(→w_t; →w_{1..t-1}) = ∑_i \Big[\ell\big(t_i, y^{(t-1)}(→x_i; →w_{1..t-1}) + y_t(→x_i; →w_t)\big)\Big] + \frac{1}{2} λ \big\|→w_t\big\|^2.$$

~~~
In the following text, we drop the parameters of $y^{(t-1)}$ and $y_t$ for
brevity.

~~~
The original idea of gradient boosting was to set
$$y_t(→x_i) ← -\frac{∂\ell\big(t_i, y^{(t-1)}(→x_i)\big)}{∂y^{(t-1)}(→x_i)}
  \textcolor{gray}{=-\frac{∂\ell\big(t_i, y\big)}{∂y}\bigg|_{y=y^{(t-1)}(→x_i)}}$$
as a direction minimizing the residual loss
~~~
and then finding a suitable constant $γ_t$, which would minimize the loss
$$∑_i \Big[\ell\big(t_i, y^{(t-1)}(→x_i) + γ_t y_t(→x_i)\big)\Big] + \frac{1}{2} λ \big\|→w_t\big\|^2.$$

---
class: middle
# Today's Lecture Objectives

After this lecture you should be able to

- Implement Decision Trees and Random Forests for classification and regression

- Explain how the splitting criterion depend on optimized loss function

- Tell how Random Forests differ from Gradient Boosted Decision Trees
