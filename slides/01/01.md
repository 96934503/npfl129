title: NPFL129, Lecture 1
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Introduction to Machine Learning

## Milan Straka

### October 07, 2019

---
section: Machine Learning
# Machine Learning

A possible definition of learning from Mitchell (1997):
>  A computer program is said to learn from experience E with respect to some
>  class of tasks T and performance measure P, if its performance at tasks in
>  T, as measured by P, improves with experience E.

~~~
- Task T
    - _classification_: assigning one of $k$ categories to a given input
    - _regression_: producing a number $x∈ℝ$ for a given input
    - _structured prediction_, _denoising_, _density estimation_, …
- Experience E
    - _supervised_: usually a dataset with desired outcomes (_labels_ or
      _targets_)
    - _unsupervised_: usually data without any annotation (raw text, raw images, …)
    - _reinforcement learning_, _semi-supervised learning_, …
- Measure P
    - _accuracy_, _error rate_, _F-score_, …

---
# Deep Learning Highlights
- Image recognition

~~~ ~
# Deep Learning Highlights
![w=60%,h=center](imagenet_recognition.jpg)

~~~ ~~
- Object detection
~~~ ~
# Deep Learning Highlights
![w=100%,v=middle](object_detection.pdf)

~~~ ~~
- Image segmentation,
~~~ ~
# Deep Learning Highlights
![w=100%,v=middle](image_segmentation.pdf)

~~~ ~~
- Human pose estimation
~~~ ~
# Deep Learning Highlights
![w=100%,v=middle](human_pose_estimation.pdf)

~~~ ~~
- Image labeling
~~~ ~
# Deep Learning Highlights
![w=75%,h=center](image_labeling.pdf)

~~~ ~~
- Visual question answering
~~~ ~
# Deep Learning Highlights
![w=70%,h=center](vqa.pdf)

~~~ ~~
- Speech recognition and generation
~~~ ~
# Deep Learning Highlights

<audio controls style="width: 32%"><source src="https://google.github.io/tacotron/publications/tacotron2/demos/washington_gen.wav"></audio>
<audio controls style="width: 32%"><source src="https://google.github.io/tacotron/publications/tacotron2/demos/columbia_gen.wav"></audio>
<audio controls style="width: 32%"><source src="https://google.github.io/tacotron/publications/tacotron2/demos/fox_question.wav"></audio>

![w=100%](tacotron_comparison.pdf)

~~~ ~~
- Lip reading
~~~ ~
# Deep Learning Highlights
![w=70%,h=center](lrw_showcase.pdf)
![w=70%,h=center](lipnet_saliency.pdf)

~~~ ~~
- Machine translation
~~~ ~
# Deep Learning Highlights
![w=44%,h=center](attention_visualization.pdf)

~~~ ~~
- Machine translation without parallel data
~~~ ~
# Deep Learning Highlights
![w=70%,h=center](umt_ideas.pdf)
![w=30%,h=center](umt_comparison.pdf)

~~~ ~~
- Chess, Go and Shogi
~~~ ~
# Deep Learning Highlights
![w=95%,h=center](a0_results.pdf)

~~~ ~~
- Multiplayer Capture the flag
~~~ ~
# Deep Learning Highlights
![w=100%,v=middle](ctf_overview.pdf)

~~~ ~~

---
class: wide
# Introduction to Machine Learning History

![w=99%,h=center](figure1_ANN_history.jpg)

---
# Machine and Representation Learning

![w=35%,h=center](machine_learning.pdf)

---
section: TL;DR
# Basic Machine Learning Settings

Assume we have an input of $→x ∈ ℝ^d$.

~~~
Then the two basic ML tasks are:
1. **regression**: The goal of a regression is to predict real-valued target
   variable $t ∈ ℝ$ of the given input.

~~~
2. **classification**: Assuming we have a fixed set of $K$ labels, the goal
   of a classification is to choose a corresponding label/class for a given
   input.
~~~
   - We can predict the class only.
~~~
   - We can predict the whole distribution of all classes probabilities.

~~~
We usually have a **training set**, which is assumed to consist of examples
of $(→x, t)$ generated independently from a **data generating distribution**.

~~~
The goal of _optimization_ is to match the training set as well as possible.

~~~
However, the goal of _machine learning_ is to perform well on _previously
unseen_ data, to achieve lowest **generalization error** or **test error**. We
typically estimate it using a **test set** of examples independent of the
training set, but generated by the same data generating distribution.

---
# Notation

- $a$, $→a$, $⇉A$, $⇶A$: scalar (integer or real), vector, matrix, tensor

- $⁇a$, $⁇→a$, $⁇⇉A$: scalar, vector, matrix random variable

~~~
- $\frac{df}{dx}$: derivative of $f$ with respect to $x$

- $\frac{∂f}{∂x}$: partial derivative of $f$ with respect to $x$

~~~
- $∇_→x f$: gradient of $f$ with respect to $→x$, i.e.,
  $\left(\frac{∂f(→x)}{∂x_1}, \frac{∂f(→x)}{∂x_2}, \ldots, \frac{∂f(→x)}{∂x_n}\right)$

---
section: Linear Regression
# Example Dataset

Assume we have the following data, generated from an underlying curve
by adding a small amount of Gaussian noise.

![w=57%,h=center](sin_data.pdf)

---
# Linear Regression

Given an input value $→x ∈ ℝ^d$, one of the simplest models to predict
a target real value is **linear regression**:
$$f(→x; →w, b) = →x^T →w + b.$$
The $→w$ are usually called _weights_ and $b$ is called _bias_.

~~~
Sometimes it is convenient not to deal with the bias separately. Instead,
we might enlarge the input vector $→x$ by padding a value 1, and consider only
$→x^T→w$, where the role of a bias is accomplished by the last weight.
Therefore, when we say “weights”, we usually mean both weights and biases.

---
# Linear Regression

Assume we have a dataset of $N$ input values $→x_1, …, →x_N$ and targets
$t_1, …, t_N$.

To find the values of weights, we usually minimize an **error function**
between the real target values and their predictions.

~~~
A popular and simple error function is _mean squared error_:
![w=40%,f=right](mse.pdf)

$$\operatorname{MSE}(→w) = \frac{1}{N} ∑_{i=1}^N \big(f(→x_i; →w) - t_i\big)^2.$$

~~~
Often, _sum of squares_
$$\frac{1}{2} ∑_{i=1}^N \big(f(→x_i; →w) - t_i\big)^2$$
is used instead, because the math comes out nicer.

---
# Linear Regression

There are several ways how to minimize the error function, but in the case of
linear regression and mean squared error, there exists an explicit solution.

Assuming $⇉X ∈ ℝ^{N×D}$ is a matrix of input values with $→x_i$ on a row $i$,
and $→t ∈ ℝ^N$ is a vector of target values, we are aiming to minimize
$$\tfrac{1}{2} ||⇉X→w - →t||^2 = \tfrac{1}{2} (⇉X→w - →t)^T (⇉X→w - →t).$$

~~~
In order to find a minimum, we can inspect values where the derivative of the
error function is zero.

$$\begin{aligned}
\frac{∂}{∂→w} \tfrac{1}{2} (⇉X→w - →t)^T (⇉X→w - →t) &= \frac{∂}{∂→w} \tfrac{1}{2} (→w^T⇉X^T - →t^T) (⇉X→w - →t) \\
  &= \tfrac{1}{2}⇉X^T(⇉X→w - →t) + \tfrac{1}{2}⇉X^T(⇉X→w - →t) \\
  &= ⇉X^T⇉X→w - ⇉X^T→t.
\end{aligned}$$

For the derivative is therefore zero when $⇉X^T⇉X→w = ⇉X^T→t$, so when
$→w = (⇉X^T⇉X)^{-1}⇉X^T→t.$

---
# Linear Regression
<div class="algorithm">

**Input**: Dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ ℝ^N$).<br>
**Output**: Weights $→w ∈ ℝ^D$ minimizing MSE of linear regression.

- $→w ← (⇉X^T⇉X)^{-1}⇉X^T→t.$
</div>

~~~
The algorithm has complexity $𝓞(ND^2)$, assuming $N≥D$.

~~~
Note that there exist better solutions which do not require the $(⇉X^T⇉X)^{-1}$
to exist (i.e., they solve directly the $⇉X^T⇉X→w = ⇉X^T→t$ equation), but they
have the same complexity.

---
# Linear Regression Example

Assume our input vectors comprise of $→x = (x^0, x^1, …, x^M)$, for $M ≥ 0$.

![w=60%,h=center](sin_lr.pdf)

---
# Linear Regression Example

To plot the error, the _root mean squared error_
$\operatorname{RMSE}=\sqrt{\operatorname{MSE}}$ is frequently used.

![w=60%,f=right](sin_errors.pdf)
~~~

The displayed error nicely illustrates two main challenges in machine learning:
- _underfitting_
- _overfitting_

---
section: Regularization
# Model Capacity
We can control whether a model underfits or overfits by modifying its _capacity_.
~~~
- representational capacity
- effective capacity

~~~
![w=80%,h=center](generalization_error.pdf)

---
# Linear Regression Overfitting

Note that employing more data also usually alleviates overfitting (the relative
capacity of the model is decreased).

![w=100%](sin_overfitting.pdf)

---
# Regularization

**Regularization** in a broad sense is any change in a machine learning
algorithm that is designed to _reduce generalization error_  but not necessarily
its training error).

~~~
$L_2$ regularization (also called weighted decay) penalizes models
with large weights:

$$\frac{1}{2} ∑_{i=1}^N \big(f(→x_i; →w) - t_i\big)^2 + \frac{λ}{2} ||→w||^2$$

![w=60%,h=center](sin_regularization.pdf)

---
# Regularizing Linear Regression

In matrix form, regularized sum of squares error for linear regression amounts
to
$$\tfrac{1}{2} ||⇉X→w - →t||^2 + \tfrac{λ}{2} ||→w||^2  = \tfrac{1}{2} (⇉X→w - →t)^T (⇉X→w - →t) + \tfrac{λ}{2} →w^t→w.$$

When repeating the same calculation as in the unregularized case, we arrive at
$$(⇉X^T⇉X + λ⇉I)→w = ⇉X^T→t,$$
where $⇉I$ is an identity matrix.

~~~
<div class="algorithm">

**Input**: Dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ ℝ^N$), constant $λ ∈ ℝ^+$.<br>
**Output**: Weights $→w ∈ ℝ^D$ minimizing MSE of regularized linear regression.

- $→w ← (⇉X^T⇉X + λ⇉I)^{-1}⇉X^T→t.$
</div>

---
# Choosing Hyperparameters

_Hyperparameters_ are not adapted by the learning algorithm itself.

Usually a **validation set** or **development set** is used to
estimate the generalization error, allowing to update hyperparameters accordingly.
If there is not enough data (well, there is **always** not enough data),
more sophisticated approaches can be used.

~~~
So far, we have seen two hyperparameters, $M$ and $λ$.

~~~
![w=45%,h=center](sin_regularization_ablation.pdf)

---
section: SGD
# Gradient Descent

Sometimes it is more practical to search for the best model weights
in an iterative/incremental/sequential fashion. Either because there is too much
data, or the direct optimization is not feasible.

~~~
![w=53%,f=right](gradient_descent.pdf)

Assuming we are minimizing an error function
$$\argmin_→w E(→w),$$
we may use _gradient descent_:
$$→w ← →w - α∇_→wE(→w)$$

~~~
The name _stochastic gradient descent_ or _online_ gradient descent
is a variant when we iteratively update the weights one training example
at a time.

---
# Gradient Descent of Linear Regression

For linear regression and sum of squares, we can update the weights as
$$→w ← →w - α∇_→wE(→w) = →w - α(→x^T→w-t)→x.$$

~~~
<div class="algorithm">

**Input**: Dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ ℝ^N$), learning rate $α ∈ ℝ^+$.<br>
**Output**: Weights $→w ∈ ℝ^D$ which hopefully minimize MSE of linear regression.

- $→w ← 0$
- for $i = 1, \ldots, n$:
  - $→w ← →w - α(→x_i^T→w-t_i)→x_i.$
</div>

---
section: Classification
# Binary Classification

Binary classification is a classification in two classes.

~~~
To extend linear regression to binary classification, we might seek
a _threshold_ and the classify an input as negative/positive
depending whether $→x^T→w$ is smaller/larger than a given threshold.

~~~
The specific threshold value does not really matter, because the
_bias_ parameter acts as a trainable threshold anyway.

---
# Perceptron

The perceptron algorithm is probably the oldest one for training
weights of a binary classification. It can be proven that it finds
weights $→w$ classifying training set with 100% accuracy if such
weights exist. Such a dataset is called _linearly separable_.

~~~
<div class="algorithm">

**Input**: Linearly separable dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ \{-1, +1\}$).<br>
**Output**: Weights $→w ∈ ℝ^D$ such that $t_i →x_i^T→w > 0$ for all $i$.

- $→w ← 0$
- until all examples are classified correctly, process example $i$:
  - $y ← →w^T→x_i$
  - if $t_i y < 0$ (incorrectly classified example):
    - $→w ← →w + t_i →x_i$
</div>

---
# Logistic Regression

Extends perceptron by
- allowing more than two classes
~~~
- producing full probability distribution.

~~~
For binary case, it employs a formula
$$\begin{aligned}
  P(C_1 | →x) &= σ(→x^t →w + →b) \\
  P(C_0 | →x) &= 1 - P(C_1 | →x),
\end{aligned}$$
where $σ$ is a _sigmoid function_
$$σ(x) = \frac{1}{1+e^{-x}}.$$

~~~
Can be trained using an SGD algorithm.

---
section: Backup
# Random Variables
A random variable $⁇x$ is a result of a random process. It can be discrete or
continuous.

~~~
## Probability Distribution
A probability distribution describes how likely are individual values a random
variable can take.

The notation $⁇x ∼ P$ stands for a random variable $⁇x$ having a distribution $P$.

~~~
For discrete variables, the probability that $⁇x$ takes a value $x$ is denoted as
$P(x)$ or explicitly as $P(⁇x = x)$.

~~~
For continuous variables, the probability that the value of $⁇x$ lies in the interval
$[a, b]$ is given by $∫_a^b p(x)\d x$.

---
# Random Variables

## Expectation
The expectation of a function $f(x)$ with respect to discrete probability
distribution $P(x)$ is defined as:
$$𝔼_{⁇x ∼ P}[f(x)] ≝ ∑_x P(x)f(x)$$

For continuous variables it is computed as:
$$𝔼_{⁇x ∼ p}[f(x)] ≝ ∫_x p(x)f(x)\d x$$

~~~
If the random variable is obvious from context, we can write only $𝔼_P[x]$
of even $𝔼[x]$.

~~~
Expectation is linear, i.e.,
$$𝔼_⁇x [αf(x) + βg(x)] = α𝔼_⁇x [f(x)] + β𝔼_⁇x [g(x)]$$

---
# Random Variables

## Variance
Variance measures how much the values of a random variable differ from its
mean $μ = 𝔼[x]$.

$$\begin{aligned}
  \Var(x) &≝ 𝔼\left[\big(x - 𝔼[x]\big)^2\right]\textrm{, or more generally} \\
  \Var(f(x)) &≝ 𝔼\left[\big(f(x) - 𝔼[f(x)]\big)^2\right]
\end{aligned}$$

~~~
It is easy to see that
$$\Var(x) = 𝔼\left[x^2 - 2x𝔼[x] + \big(𝔼[x]\big)^2\right] = 𝔼\left[x^2\right] - \big(𝔼[x]\big)^2.$$

~~~
Variance is connected to $E[x^2]$, a _second moment_ of a random
variable – it is in fact a _centered_ second moment.

---
# Common Probability Distributions
## Bernoulli Distribution
The Bernoulli distribution is a distribution over a binary random variable.
It has a single parameter $φ ∈ [0, 1]$, which specifies the probability of the random
variable being equal to 1.

~~~
$$\begin{aligned}
  P(x) &= φ^x (1-φ)^{1-x} \\
  𝔼[x] &= φ \\
  \Var(x) &= φ(1-φ)
\end{aligned}$$

~~~
## Categorical Distribution
Extension of the Bernoulli distribution to random variables taking one of $k$ different
discrete outcomes. It is parametrized by $→p ∈ [0, 1]^k$ such that $∑_{i=1}^{k} p_{i} = 1$.
$$\begin{aligned}
  P(→x) &= ∏\nolimits_i^k p_i^{x_i} \\
  𝔼[x_i] &= p_i, \Var(x_i) = p_i(1-p_i) \\
\end{aligned}$$

---
# Information Theory

## Self Information

Amount of _surprise_ when a random variable is sampled.
~~~
- Should be zero for events with probability 1.
~~~
- Less likely events are more surprising.
~~~
- Independent events should have _additive_ information.

~~~
$$I(x) ≝ -\log P(x) = \log \frac{1}{P(x)}$$

~~~
## Entropy

Amount of _surprise_ in the whole distribution.
$$H(P) ≝ 𝔼_{⁇x∼P}[I(x)] = -𝔼_{⁇x∼P}[\log P(x)]$$

~~~
- for discrete $P$: $H(P) = -∑_x P(x) \log P(x)$
- for continuous $P$: $H(P) = -∫ P(x) \log P(x)\,\mathrm dx$

---
# Information Theory

## Cross-Entropy

$$H(P, Q) ≝ -𝔼_{⁇x∼P}[\log Q(x)]$$

~~~
- Gibbs inequality
    - $H(P, Q) ≥ H(P)$
    - $H(P) = H(P, Q) ⇔ P = Q$
~~~
    - Proof: Using Jensen's inequality, we get
      $$∑_x P(x) \log \frac{Q(x)}{P(x)} ≤ \log ∑_x P(x) \frac{Q(x)}{P(x)} = \log ∑_x Q(x) = 0.$$
~~~
    - Corollary: For a categorical distribution with $n$ outcomes, $H(P) ≤ \log n$,
    because for $Q(x) = 1/n$ we get $H(P) ≤ H(P, Q) = -∑_x P(x) \log Q(x) = \log n.$
~~~
- generally $H(P, Q) ≠ H(Q, P)$

---
# Information Theory

## Kullback-Leibler Divergence (KL Divergence)

Sometimes also called _relative entropy_.

$$D_\textrm{KL}(P || Q) ≝ H(P, Q) - H(P) = 𝔼_{⁇x∼P}[\log P(x) - \log Q(x)]$$

~~~
- consequence of Gibbs inequality: $D_\textrm{KL}(P || Q) ≥ 0$
- generally $D_\textrm{KL}(P || Q) ≠ D_\textrm{KL}(Q || P)$

---
# Nonsymmetry of KL Divergence

![w=100%,v=middle](kl_nonsymmetry.pdf)

---
# Common Probability Distributions
## Normal (or Gaussian) Distribution
Distribution over real numbers, parametrized by a mean $μ$ and variance $σ^2$:
$$𝓝(x; μ, σ^2) = \sqrt{\frac{1}{2πσ^2}} \exp \left(-\frac{(x - μ)^2}{2σ^2}\right)$$

For standard values $μ=0$ and $σ^2=1$ we get $𝓝(x; 0, 1) = \sqrt{\frac{1}{2π}} e^{-\frac{x^2}{2}}$.

![w=45%,h=center](normal_distribution.pdf)

---
# Why Normal Distribution

## Central Limit Theorem
The sum of independent identically distributed random variables
with finite variance converges to normal distribution.

~~~
## Principle of Maximum Entropy
Given a set of constraints, a distribution with maximal entropy fulfilling the
constraints can be considered the most general one, containing as little
additional assumptions as possible.

~~~
Considering distributions with a given mean and variance, it can be proven
(using variational inference) that such a distribution with _maximal entropy_
is exactly the normal distribution.
