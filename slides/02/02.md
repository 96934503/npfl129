title: NPFL129, Lecture 2
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Linear Regression II, SGD, Perceptron

## Milan Straka

### October 12, 2020

---
# Linear Regression Example

Assume our input vectors comprise of $→x = (x^0, x^1, …, x^M)$, for $M ≥ 0$.

![w=60%,h=center](../01/sin_lr.svgz)

---
# Linear Regression Example

To plot the error, the _root mean squared error_
$\operatorname{RMSE}=\sqrt{\operatorname{MSE}}$ is frequently used.

![w=60%,f=right](../01/sin_errors.svgz)
~~~

The displayed error nicely illustrates two main challenges in machine learning:
- _underfitting_
- _overfitting_

---
section: Regularization
# Model Capacity
We can control whether a model underfits or overfits by modifying its _capacity_.
~~~
- representational capacity
- effective capacity

~~~
![w=80%,h=center](generalization_error.svgz)

---
# Linear Regression Overfitting

Note that employing more data also usually alleviates overfitting (the relative
capacity of the model is decreased).

![w=100%](sin_overfitting.svgz)

---
# Regularization

**Regularization** in a broad sense is any change in a machine learning
algorithm that is designed to _reduce generalization error_  but not necessarily
its training error).

~~~
$L_2$ regularization (also called weighted decay) penalizes models
with large weights:

$$\frac{1}{2} ∑_{i=1}^N \big(f(→x_i; →w) - t_i\big)^2 + \frac{λ}{2} ||→w||^2$$

![w=60%,h=center](sin_regularization.svgz)

---
# Regularizing Linear Regression

In matrix form, regularized sum of squares error for linear regression amounts
to
$$\tfrac{1}{2} ||⇉X→w - →t||^2 + \tfrac{λ}{2} ||→w||^2.$$

When repeating the same calculation as in the unregularized case, we arrive at
$$(⇉X^T⇉X + λ⇉I)→w = ⇉X^T→t,$$
where $⇉I$ is an identity matrix.

~~~
<div class="algorithm">

**Input**: Dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ ℝ^N$), constant $λ ∈ ℝ^+$.<br>
**Output**: Weights $→w ∈ ℝ^D$ minimizing MSE of regularized linear regression.

- $→w ← (⇉X^T⇉X + λ⇉I)^{-1}⇉X^T→t.$
</div>

---
section: Hyperparameters
# Choosing Hyperparameters

_Hyperparameters_ are not adapted by the learning algorithm itself.

Usually a **validation set** or **development set** is used to
estimate the generalization error, allowing to update hyperparameters accordingly.
If there is not enough data (well, there is **always** not enough data),
more sophisticated approaches can be used.

~~~
So far, we have seen two hyperparameters, $M$ and $λ$.

~~~
![w=45%,h=center](sin_regularization_ablation.svgz)
