title: NPFL129, Lecture 2
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Linear Regression II, SGD

## Milan Straka

### October 11, 2021

---
section: Refresh
# Linear Regression

Given an input value $→x ∈ ℝ^D$, **linear regression** computes predictions as:
$$y(→x; →w, b) = x_1 w_1 + x_2 w_2 + … + x_D w_D + b = ∑_{i=1}^D x_i w_i + b = →x^T →w + b.$$
The _bias_ $b$ can be considered one of the _weights_ $→w$ if convenient.

~~~
We train the weights by minimizing an **error function** between the real target
values and their predictions, notably _sum of squares_:
$$\frac{1}{2} ∑_{i=1}^N \big(y(→x_i; →w) - t_i\big)^2$$

~~~
There are several ways how to minimize it, but in our case, there exists an
explicit solution:
$$→w = (⇉X^T⇉X)^{-1}⇉X^T→t.$$

---
# Linear Regression Example

Assume our input vectors comprise of $→x = (x^0, x^1, …, x^M)$, for $M ≥ 0$.

![w=60%,h=center](../01/sin_lr.svgz)

---
# Linear Regression Example

To plot the error, the _root mean squared error_
$\operatorname{RMSE}=\sqrt{\operatorname{MSE}}$ is frequently used.

![w=60%,f=right](../01/sin_errors.svgz)
~~~

The displayed error nicely illustrates two main challenges in machine learning:
- _underfitting_
- _overfitting_

---
section: Regularization
# Model Capacity
We can control whether a model underfits or overfits by modifying its _capacity_.
~~~
- representational capacity
- effective capacity

~~~
![w=80%,h=center](generalization_error.svgz)

---
# Linear Regression Overfitting

Note that employing more data also usually alleviates overfitting (the relative
capacity of the model is decreased).

![w=100%](sin_overfitting.svgz)

---
# Regularization

**Regularization** in a broad sense is any change in a machine learning
algorithm that is designed to _reduce generalization error_ (but not necessarily
its training error).

~~~
$L_2$ regularization (also called weighted decay) penalizes models
with large weights:

$$\frac{1}{2} ∑_{i=1}^N \big(y(→x_i; →w) - t_i\big)^2 + \frac{λ}{2} \|→w\|^2$$

![w=66%](sin_regularization.svgz)
~~~
![w=32.5%](sin_regularization_ablation.svgz)

---
# Regularizing Linear Regression

In matrix form, regularized sum of squares error for linear regression amounts
to
$$\tfrac{1}{2} \|⇉X→w - →t\|^2 + \tfrac{λ}{2} \|→w\|^2.$$

When repeating the same calculation as in the unregularized case, we arrive at
$$(⇉X^T⇉X + λ⇉I)→w = ⇉X^T→t,$$
where $⇉I$ is an identity matrix.

~~~
<div class="algorithm">

**Input**: Dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ ℝ^N$), constant $λ ∈ ℝ^+$.<br>
**Output**: Weights $→w ∈ ℝ^D$ minimizing MSE of regularized linear regression.

- $→w ← (⇉X^T⇉X + λ⇉I)^{-1}⇉X^T→t.$
</div>
