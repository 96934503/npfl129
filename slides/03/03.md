title: NPFL129, Lecture 3
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Perceptron and Logistic Regression

## Milan Straka

### October 19, 2020

---
section: Features
# Features

Recall that the _input_ instance values are usually the raw observations and are
given. However, we might extend them suitably before running a machine learning
algorithm, especially if the algorithm is linear or otherwise limited and cannot
represent arbitrary function. Such instance representations is called _features_.

~~~
We already saw this in the example from the previous lecture, where even if
our training examples were $x$ and $t$, we performed the linear regression
using features $(x^0, x^1, …, x^M)$:
![w=40%,h=center](../01/sin_lr.svgz)

---
# Features

Generally, it would be best if we have machine learning algorithms processing
only the raw inputs. However, many algorithms are capable of representing
only a limited set of functions (for example linear ones), and in that case,
_feature engineering_ plays a major part in the final model performance.
Feature engineering is a process of constructing features from raw inputs.

Commonly used features are:
~~~
- **polynomial features** of degree $p$: Given features $(x_1, x_2, …, x_D)$, we
  might consider _all_ products of $p$ input values. Therefore, polynomial
  features of degree 2 would consist of $x_i^2 \,∀i$ and of $x_i x_j \,∀i≠j$.

~~~
- **categorical one-hot features**: Assume for example that a day in a week is
  represented on the input as an integer value of 1 to 7, or a breed of a dog is
  expressed as an integer value of 0 to 366.
~~~
  Using these integral values as input to linear regression makes little sense
  – instead it might be better to learn weights for individual days in a week or
  for individual dog breeds.
~~~
  We might therefore represent input classes by binary indicators for every
  class, giving rise to **one-hot** representation, where input integral value
  $1 ≤ v ≤ L$ is represented as $L$ binary values, which are all zero except for
  the $v$-th one, which is one.

---
section: CV
# Cross-Validation

We already talked about a **train set** and a **test set**. Given that the main
goal of machine learning is to perform well on unseen data, the test set must
not be used during training nor hyperparameter selection. Ideally, it is hidden
to us altogether.

~~~
Therefore, to evaluate a machine learning model (for example to select model
architecture, features, or hyperparameter value), we normally need the
**validation** or a **development** set.

~~~
However, using a single development set might give us noisy results. To obtain
less noisy results (i.e., with smaller variance), we can use
**cross-validation**.

~~~
![w=48%,f=right](k-fold_cross_validation.svgz)

In cross-validation, we choose multiple validation sets from the training data,
and for every one, we train a model on the rest of the training data and
evaluate on the chosen validation sets. A commonly used strategy to choose
the validation sets is called **k-fold cross-validation**. Here the training set is partitioned
into $k$ subsets of approximately the same size, and each subset takes turn
to play a role of a validation set.

---
# Cross-Validation

An extreme case of the **k-fold cross-validation** is **leave-one-out
cross-validation**, where every element is considered a separate validation
set.

~~~
Computing leave-one-out cross-validation is usually extremely inefficient for
larger training sets, but in case of linear regression it can be evaluated
efficiently.
